{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from scipy.spatial.distance import cosine\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# The following data science skill sets are modified from \n",
    "program_languages = ['bash','r','python','java','c++','ruby','perl','matlab','javascript','scala','php']\n",
    "analysis_software = ['excel','tableau','sas','spss','d3','saas','pandas','numpy','scipy','sps','spotfire','scikit','splunk','power','h2o']\n",
    "ml_framework = ['pytorch','tensorflow','caffe','caffe2','cntk','mxnet','paddle','keras','bigdl']\n",
    "bigdata_tool = ['hadoop','mapreduce','spark','pig','hive','shark','oozie','zookeeper','flume','mahout','etl']\n",
    "ml_platform = ['aws','azure','google','ibm']\n",
    "methodology = ['agile','devops','scrum']\n",
    "databases = ['sql','nosql','hbase','cassandra','mongodb','mysql','mssql','postgresql','oracle','rdbms','bigquery']\n",
    "overall_skills_dict = program_languages + analysis_software + ml_framework + bigdata_tool + databases + ml_platform + methodology\n",
    "education = ['master','phd','undergraduate','bachelor','mba']\n",
    "overall_dict = overall_skills_dict + education\n",
    "# specify the length of each minhash vector\n",
    "N = 32\n",
    "max_val = (2**8)-1\n",
    "#Create N tuples that will serve as permutation functions.These permutation values are used to hash all input sets.\n",
    "perms = [ (randint(0,max_val), randint(0,max_val)) for i in range(N)]\n",
    "#Initialize a sample minhash vector of length N.\n",
    "#Each record will be represented by its own vec.\n",
    "vec = [float('inf') for i in range(N)]\n",
    "\n",
    "\n",
    "class skill_keyword_match:\n",
    "    jobs_info_df = None\n",
    "    def __init__(self, jobs_list):\n",
    "        '''\n",
    "        Initialization - converts list to DataFrame\n",
    "        Input: \n",
    "            jobs_list (list): a list of all jobs info\n",
    "        Output: \n",
    "            None\n",
    "        '''\n",
    "        self.jobs_info_df = pd.DataFrame(jobs_list) \n",
    "        self.jobs_info_df.rename(columns = {'0':'job_title','1':'company_name', '2':'location','3':'job_desc'})\n",
    "        print(\"Dataset:\\n\")\n",
    "        print(self.jobs_info_df.head(5))\n",
    "        \n",
    "        \n",
    "    def keywords_extract(self, text): \n",
    "        '''\n",
    "        Tokenize webpage text and extract keywords\n",
    "        Input: \n",
    "            text (str): text to extract keywords from\n",
    "        Output: \n",
    "            keywords (list): keywords extracted and filtered by pre-defined dictionary\n",
    "        '''        \n",
    "        text = re.sub(\"[^a-zA-Z+3]\",\" \", text) \n",
    "        text = text.lower().split()\n",
    "        stops = set(stopwords.words(\"english\")) \n",
    "        #filter out stop words in english language\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = list(set(text))\n",
    "        #keywords from the pre-defined skill dictionary\n",
    "        keywords = [str(word) for word in text if word in overall_dict]\n",
    "        return keywords\n",
    " \n",
    "    def keywords_count(self, keywords, counter): \n",
    "        '''\n",
    "        Count frequency of keywords\n",
    "        Input: \n",
    "            keywords (list): list of keywords\n",
    "            counter (Counter)\n",
    "        Output: \n",
    "            keyword_count (DataFrame index:keyword value:count)\n",
    "        '''           \n",
    "        keyword_count = pd.DataFrame(columns = ['Freq'])\n",
    "        for each_word in keywords: \n",
    "            keyword_count.loc[each_word] = {'Freq':counter[each_word]}\n",
    "        return keyword_count\n",
    "    \n",
    "    def exploratory_data_analysis(self):\n",
    "        '''\n",
    "        Exploratory data analysis\n",
    "        Input: \n",
    "            None\n",
    "        Output: \n",
    "            None\n",
    "        '''         \n",
    "        # Create a counter of keywords\n",
    "        doc_freq = Counter() \n",
    "        f = [doc_freq.update(item) for item in self.jobs_info_df['keywords']]\n",
    "        \n",
    "        #Pre-defined skillset vocabulary in Counter\n",
    "        overall_skills_df = self.keywords_count(overall_skills_dict, doc_freq)\n",
    "        #Calculate percentage of required skills in all jobs\n",
    "        overall_skills_df['Freq_perc'] = (overall_skills_df['Freq'])*100/self.jobs_info_df.shape[0]\n",
    "        overall_skills_df = overall_skills_df.sort_values(by='Freq_perc', ascending=False)  \n",
    "        #Make bar plot \n",
    "        plt.figure(figsize=(14,8))\n",
    "        overall_skills_df.iloc[0:30, overall_skills_df.columns.get_loc('Freq_perc')].plot.bar()\n",
    "        plt.title('Percentage of Required Data Skills in Data Scientist Job Posts')\n",
    "        plt.ylabel('Percentage Required in Jobs (%)')\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.show()\n",
    "        \n",
    "         \n",
    "        #Education requirements\n",
    "        education_df = self.keywords_count(education, doc_freq)\n",
    "        #Merge undergrad with bachelor\n",
    "        education_df.loc['bachelor','Freq'] = education_df.loc['bachelor','Freq'] + education_df.loc['undergraduate','Freq'] \n",
    "        education_df.drop(labels='undergraduate', axis=0, inplace=True)\n",
    "        #Calculate percentage of required skills in all jobs\n",
    "        education_df['Freq_perc'] = (education_df['Freq'])*100/self.jobs_info_df.shape[0] \n",
    "        education_df = education_df.sort_values(by='Freq_perc', ascending=False)  \n",
    "        \n",
    "        \n",
    "    def get_jaccard_sim(self, x_set, y_set): \n",
    "        '''\n",
    "        Jaccard similarity or intersection over union measures similarity \n",
    "        between finite sample sets,  and is defined as size of intersection \n",
    "        divided by size of union of two sets. \n",
    "        \n",
    "        Input: \n",
    "            x_set (set)\n",
    "            y_set (set)\n",
    "        Output: \n",
    "            Jaccard similarity score\n",
    "        '''         \n",
    "        intersection = x_set.intersection(y_set)\n",
    "        return float(len(intersection)) / (len(x_set) + len(y_set) - len(intersection))\n",
    "    \n",
    "    \n",
    "    def cal_similarity(self, resume_keywords, location=None):\n",
    "        '''\n",
    "        Calculate similarity between keywords from resume and job posts\n",
    "        Input: \n",
    "            resume_keywords (list): resume keywords\n",
    "            location (str): city to search jobs\n",
    "        Output: \n",
    "            top_match (DataFrame): top job matches\n",
    "        '''     \n",
    "        num_jobs_return = 20\n",
    "        similarity = []\n",
    "        j_info = self.jobs_info_df\n",
    "        if j_info.shape[0] < num_jobs_return:        \n",
    "            num_jobs_return = j_info.shape[0]  \n",
    "        for job_skills in j_info['keywords']:\n",
    "            similarity.append(self.get_jaccard_sim(set(resume_keywords), set(job_skills)))\n",
    "        j_info['similarity'] = similarity\n",
    "        top_match = j_info.sort_values(by='similarity', ascending=False).head(num_jobs_return)        \n",
    "        # Return top matched jobs\n",
    "        return top_match\n",
    "      \n",
    "        \n",
    "    def extract_jobs_keywords(self):\n",
    "        '''\n",
    "        Extract skill keywords from job descriptions and add a new column \n",
    "        Input: \n",
    "            None\n",
    "        Output: \n",
    "            None\n",
    "        \n",
    "        '''\n",
    "        self.jobs_info_df.head(5)\n",
    "        self.jobs_info_df['keywords'] = [self.keywords_extract(job_desc) for job_desc in self.jobs_info_df[3]]\n",
    "        \n",
    "        \n",
    "    def extract_resume_keywords(self, resume_pdf): \n",
    "        '''\n",
    "        Extract key skills from a resume \n",
    "        Input: \n",
    "            resume_pdf (str): path to resume PDF file\n",
    "        Output: \n",
    "            resume_skills (DataFrame index:keyword value:count): keywords counts\n",
    "        ''' \n",
    "        #Open resume PDF\n",
    "        resume_file = open(resume_pdf, 'rb')\n",
    "        #Creating a pdf reader object\n",
    "        resume_reader = PyPDF2.PdfFileReader(resume_file)\n",
    "        #Reading each page in PDF\n",
    "        resume_content = [resume_reader.getPage(x).extractText() for x in range(resume_reader.numPages)]\n",
    "        # Extract key skills from each page\n",
    "        resume_keywords = [self.keywords_extract(page) for page in resume_content]\n",
    "        #Count keywords\n",
    "        resume_freq = Counter() \n",
    "        f = [resume_freq.update(item) for item in resume_keywords] \n",
    "        #Resume skill keywords counts\n",
    "        resume_skills = self.keywords_count(overall_skills_dict, resume_freq)\n",
    "        \n",
    "        return(resume_skills[resume_skills['Freq']>0])\n",
    "            \n",
    "    def calculate_minhash(self, resume_keywords):\n",
    "        num_jobs_return = 10\n",
    "        j_info = self.jobs_info_df\n",
    "        minhash_similarity =[]\n",
    "        \n",
    "        for job_skills in j_info['keywords']:\n",
    "            #Specify some input sets\n",
    "            data_resume = set(resume_keywords)\n",
    "            data_job_keywords = set(job_skills)\n",
    "            #Minhash vectors for each input set\n",
    "            vec1 = self.minhash(data_resume)\n",
    "            vec2 = self.minhash(data_job_keywords)\n",
    "\n",
    "            #Dividing both vectors by their max values to scale values {0:1}\n",
    "            vec1 = np.array(vec1) / max(vec1)\n",
    "            vec2 = np.array(vec2) / max(vec2)\n",
    "            cos_sim = 1 - cosine(vec1, vec2)\n",
    "            minhash_similarity.append(cos_sim)\n",
    "            #Measuring the similarity between the vectors using cosine similarity\n",
    "            print( '\\n Minhash using similarity:', cos_sim )\n",
    "            \n",
    "        j_info['Minhash_Similarity'] = minhash_similarity\n",
    "        top_match_minhash = j_info.sort_values(by='similarity', ascending=False).head(num_jobs_return)        \n",
    "        #Return top matched jobs\n",
    "        return top_match_minhash\n",
    "    \n",
    "    def minhash(self, s ,prime=4294967311):\n",
    "        vec = [float('inf') for i in range(N)]\n",
    "        \n",
    "        for val in s:\n",
    "            if not isinstance(val, int): val = hash(val)\n",
    "            for perm_idx, perm_vals in enumerate(perms):\n",
    "                a,b = perm_vals\n",
    "                output = (a * val + b) % prime\n",
    "                if vec[perm_idx]>output:\n",
    "                    vec[perm_idx] = output\n",
    "        return vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
