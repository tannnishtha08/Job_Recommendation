{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6394a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time, os\n",
    "\n",
    "\n",
    "#Collecting maximum results of 500 \n",
    "max_results_per_city = 500\n",
    "# Number of jobs show on each result page.\n",
    "page_record_limit = 50\n",
    "num_pages = int(max_results_per_city/page_record_limit)\n",
    "\n",
    "def get_jobs_info(search_location):\n",
    "    '''\n",
    "    Scrape from web or read from saved file\n",
    "    Input: \n",
    "        search_location - search job in a certain city. Input from command line.\n",
    "    Output: \n",
    "        jobs_info - a list that has info of each job i.e. link, location, title, company,desc\n",
    "    '''\n",
    "    exists = os.path.isfile(JOBS_INFO_JSON_FILE)\n",
    "    jobs_info = web_scrape(search_location)\n",
    "    if exists:\n",
    "        with open(JOBS_INFO_JSON_FILE, 'r') as fp:\n",
    "            jobs_info = json.load(fp)            \n",
    "    else:\n",
    "        jobs_info = web_scrape(search_location)\n",
    "        \n",
    "    return jobs_info\n",
    "        \n",
    "def web_scrape(search_location):\n",
    "    '''\n",
    "    Scrape jobs from indeed.ca\n",
    "    Input: \n",
    "        search_location - search job in a certain city. Input from commond line.\n",
    "    Output: \n",
    "        jobs_info - a list that has info of each job i.e. link, location, title, company, salary, desc\n",
    "    '''\n",
    "    # urls of all jobs\n",
    "    job_links = []\n",
    "    # Record time for web scraping\n",
    "    # start time\n",
    "    start = time.time() \n",
    "    # Launch webdriver\n",
    "    driver = webdriver.Chrome(WEBDRIVER_PATH)\n",
    "    job_locations = JOB_LOCATIONS\n",
    "    # If search location is defined, only search that location\n",
    "    if (len(search_location) > 0):\n",
    "        job_locations = [search_location]\n",
    "        \n",
    "    # Extract all job urls \n",
    "    for location in job_locations: \n",
    "        url = 'https://www.indeed.com/jobs?q='+ JOB_SEARCH_WORDS + '&l=' \\\n",
    "        + location + '&limit=' + str(page_record_limit) + '&fromage='+ str(DAY_RANGE)\n",
    "        # Set timeout\n",
    "        driver.set_page_load_timeout(80)\n",
    "        webdriver.DesiredCapabilities.CHROME[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  \n",
    "        for i in range(num_pages):            \n",
    "            try:\n",
    "                # Each job on the page's its url\n",
    "                from selenium.webdriver.common.by import By\n",
    "                job_names = []\n",
    "                company_names = []\n",
    "                job_locations =[]\n",
    "                job_descs = []\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//h2[@class='jobTitle css-1h4a4n5 eu4oa1w0']\"):\n",
    "                    job_name = job_each.text\n",
    "                    job_names.append(job_name)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//a[@data-tn-element='companyName']\"):\n",
    "                    company_name = job_each.text\n",
    "                    company_names.append(company_name)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//div[@class ='companyLocation']\"):\n",
    "                    job_location = job_each.text\n",
    "                    job_locations.append(job_location)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//div[@id = 'jobDescriptionText']\"):\n",
    "                    job_desc = job_each.text\n",
    "                    job_descs.append(job_desc)\n",
    "                cd = [job_each for job_each in driver.find_elements(By.XPATH, \"//div[@class = 'slider_container css-g7s71f eu4oa1w0']\")]\n",
    "                \n",
    "                result = []\n",
    "                for i in range(5):\n",
    "                    driver.get(url)\n",
    "                    time.sleep(5)  \n",
    "                    cd = [job_each for job_each in driver.find_elements(By.XPATH, \"//div[@class = 'slider_container css-g7s71f eu4oa1w0']\")]\n",
    "                    if i<len(cd):\n",
    "                        cd[i].click()\n",
    "                        time.sleep(5) \n",
    "                        result.append([jd.text for jd in driver.find_elements(By.XPATH, \"//div[@id = 'jobDescriptionText']\")])\n",
    "                job_desc = [i[0] for i in result]\n",
    "                # print(\"\\n\\nresult:\",result)\n",
    "                # slider_container css-g7s71f eu4oa1w0\n",
    "                # print(\"job_names: \\n\", job_names)\n",
    "                # print(\"company_names: \\n\", company_names)\n",
    "                # print(\"location:\\n\", job_locations)\n",
    "                # print(\"job_Desc:\\n\", job_descs)\n",
    "                zipped = zip(job_names, company_names, job_locations, job_desc)\n",
    "                zipped_list = list(zipped)\n",
    "                # print(zipped_list)\n",
    "                print ('scraping {} page {}'.format(location, i+1))\n",
    "                # Go next page\n",
    "                driver.find_element(By.LINK_TEXT,'Next Â»').click()\n",
    "            except NoSuchElementException:\n",
    "                # If nothing find, we are at the end of all returned results\n",
    "                print (\"{} finished\".format(location))\n",
    "                break        \n",
    "            time.sleep(3)\n",
    "    with open(JOBS_INFO_JSON_FILE, 'w') as fp:\n",
    "        json.dump(zipped_list, fp)\n",
    "    # Close and quit webdriver\n",
    "    driver.quit()    \n",
    "    end = time.time() # end time\n",
    "    # Calculate web scaping time\n",
    "    scaping_time = (end-start)/60.\n",
    "    print('Took {0:.2f} minutes scraping {1:d} data scientist jobs'.format(scaping_time, len(zipped_list)))\n",
    "    return zipped_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
